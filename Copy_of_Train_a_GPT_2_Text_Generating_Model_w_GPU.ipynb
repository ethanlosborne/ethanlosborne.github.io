{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Train a GPT-2 Text-Generating Model w/ GPU",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ethanlosborne/ethanlosborne.github.io/blob/master/Copy_of_Train_a_GPT_2_Text_Generating_Model_w_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "#  Train a GPT-2 Text-Generating Model w/ GPU For Free \n",
        "\n",
        "by [Max Woolf](http://minimaxir.com)\n",
        "\n",
        "*Last updated: November 10th, 2019*\n",
        "\n",
        "Retrain an advanced text generating neural network on any text dataset **for free on a GPU using Collaboratory** using `gpt-2-simple`!\n",
        "\n",
        "For more about `gpt-2-simple`, you can visit [this GitHub repository](https://github.com/minimaxir/gpt-2-simple). You can also read my [blog post](https://minimaxir.com/2019/09/howto-gpt2/) for more information how to use this notebook!\n",
        "\n",
        "\n",
        "To get started:\n",
        "\n",
        "1. Copy this notebook to your Google Drive to keep it and save your changes. (File -> Save a Copy in Drive)\n",
        "2. Make sure you're running the notebook in Google Chrome.\n",
        "3. Run the cells below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5d8aaa7-297d-4013-b4c1-6398ae7d0536"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE"
      },
      "source": [
        "## GPU\n",
        "\n",
        "Colaboratory uses either a Nvidia T4 GPU or an Nvidia K80 GPU. The T4 is slightly faster than the old K80 for training GPT-2, and has more memory allowing you to train the larger GPT-2 models and generate more text.\n",
        "\n",
        "You can verify which GPU is active by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67b6e069-9787-45ab-a4f3-045bd83274be"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Feb  7 02:05:01 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS"
      },
      "source": [
        "## Downloading GPT-2\n",
        "\n",
        "If you're retraining a model on new text, you need to download the GPT-2 model first. \n",
        "\n",
        "There are three released sizes of GPT-2:\n",
        "\n",
        "* `124M` (default): the \"small\" model, 500MB on disk.\n",
        "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
        "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
        "* `1558M`: the \"extra large\", true model. Will not work if a K80 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n",
        "\n",
        "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below.\n",
        "\n",
        "The next cell downloads it from Google Cloud Storage and saves it in the Colaboratory VM at `/models/<model_name>`.\n",
        "\n",
        "This model isn't permanently saved in the Colaboratory VM; you'll have to redownload it if you want to retrain it at a later time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e42822ca-0240-4191-d682-11979a81d1b4"
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"124M\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 445Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 116Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 262Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:04, 117Mit/s]                                   \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 273Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 165Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 126Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN"
      },
      "source": [
        "## Mounting Google Drive\n",
        "\n",
        "The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n",
        "\n",
        "Running this cell (which will only work in Colaboratory) will mount your personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5a0300c-6126-4b6a-88d2-c6c2d6121a37"
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT__brhBCvJu"
      },
      "source": [
        "## Uploading a Text File to be Trained to Colaboratory\n",
        "\n",
        "In the Colaboratory Notebook sidebar on the left of the screen, select *Files*. From there you can upload files:\n",
        "\n",
        "![alt text](https://i.imgur.com/TGcZT4h.png)\n",
        "\n",
        "Upload **any smaller text file**  (<10 MB) and update the file name in the cell below, then run the cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll"
      },
      "source": [
        "file_name = \"output2.txt\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeeSKtNWUedE"
      },
      "source": [
        "If your text file is larger than 10MB, it is recommended to upload that file to Google Drive first, then copy that file from Google Drive to the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS"
      },
      "source": [
        "gpt2.copy_file_from_gdrive(file_name)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3"
      },
      "source": [
        "## Finetune GPT-2\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6404f505-3a40-4cd4-bfb2-e88002fbbf12"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='124M',\n",
        "              steps=1000,\n",
        "              restore_from='fresh',\n",
        "              run_name='run1',\n",
        "              print_every=10,\n",
        "              sample_every=200,\n",
        "              save_every=500\n",
        "              )"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 181199 tokens\n",
            "Training...\n",
            "[10 | 29.00] loss=3.57 avg=3.57\n",
            "[20 | 50.98] loss=3.06 avg=3.31\n",
            "[30 | 73.37] loss=3.27 avg=3.30\n",
            "[40 | 96.28] loss=3.32 avg=3.30\n",
            "[50 | 119.80] loss=3.50 avg=3.34\n",
            "[60 | 143.23] loss=2.68 avg=3.23\n",
            "[70 | 166.36] loss=2.96 avg=3.19\n",
            "[80 | 189.59] loss=3.03 avg=3.17\n",
            "[90 | 212.99] loss=2.73 avg=3.12\n",
            "[100 | 236.34] loss=2.73 avg=3.08\n",
            "[110 | 259.64] loss=2.59 avg=3.03\n",
            "[120 | 282.95] loss=2.95 avg=3.03\n",
            "[130 | 306.28] loss=2.65 avg=3.00\n",
            "[140 | 329.66] loss=2.42 avg=2.95\n",
            "[150 | 353.06] loss=2.45 avg=2.92\n",
            "[160 | 376.41] loss=1.99 avg=2.85\n",
            "[170 | 399.77] loss=2.28 avg=2.82\n",
            "[180 | 423.17] loss=2.02 avg=2.77\n",
            "[190 | 446.54] loss=1.84 avg=2.72\n",
            "[200 | 469.89] loss=1.70 avg=2.66\n",
            "======== SAMPLE 1 ========\n",
            "AR\n",
            "So the battle rages on. The City’s elite take to the streets, searching out those in need. Whether through vigilantes, patrols, or even the City itself, the Vandal world is torn - atomized. Will we reclaim our ideals, or must we let our enemies out from under Earth and the worlds we call home?\n",
            "“The City is slipping. We have to find a better plan, or we risk turning the tide of things.” - Ikora Rey\n",
            "“The war is coming. Whatever goes, can’t go.  It might not be us, but the Vandals are tearing our City apart. Do you understand? The Vandals are ripping our Vandals apart? Do you need help believing them? Do you need to talk to any Guardian who can give you insight into the Vandal world? The answers are many – yes, yes, yes, yes, yes! Come talk to me, and I will talk to a Guardian’s sword. Your Light will shine on the ground and you will feel the cold heat of existence burning inside you.” - Ikora Rey\n",
            "\n",
            "\n",
            "\n",
            "The City\n",
            "\"Some in the Vault dwell in a world beyond our own. No one else sees this.\"\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The Tower\n",
            "\"Until the Vault spreads secrets of its own, Guardians watch closely - using what Guardians call 'the Towering Belt.' These powerful traps isolate and warn against the wild creatures that flow beneath the Vault, keeping us safe from harm.\n",
            "The Towering Belt is a secret but powerful weapon: Guardians rely on it to keep their Ether deposits sharp and ready to go when the time comes.\"\n",
            "\n",
            "\n",
            "Wanted: Cayde-6\n",
            "Description: An organized crime gang operating from Vault-Tec facilities in the heart of the Cosmodrome.\n",
            "I am now aware of a report on the network that may reveal major new developments.\n",
            "I have decided not to pursue any further inquiries. The Speaker has directed me to the Speaker's Table of Events, Speaker.  It is I, Speaker, who has decided the matter of the gang's fate.\n",
            "The Speaker has directed me to the Record Book for the file on the Warlocks that appears to contain notes on a number of events. This event records a discussion among the Warlocks concerning the nature of the Vault, the nature and purpose of the Vault, and the nature and purpose of the Tower. All pertinent parameters were converted from the standard Cryptography protocol to conform to the new protocols.\n",
            "I agree with the Speaker's assessment. My belief is that the discussion has now received extensive attention in the past year or so. There is very little public expression on the matter but the general public seems to be aware of a very serious informational issue.\n",
            "I am aware of the unauthorized nature of this report but, as Speaker of the House of Ether, I will not allow any further behavior that would not comply with the strictures of this separation between good and evil.\n",
            "I hereby resign as Speaker of the House of Ether and accept the rank of Major.\n",
            "\n",
            "\n",
            "\n",
            "Ghost Fragment: The City Age\n",
            "\"There is something to be done.  Hunt them all down, or send them on their own missions. It will cost a lot of Ether; even without the Vault, they will need to keep recruiting.  They won't make it to the Vault, but it is worth the effort.\"\n",
            "This was the message sent to Ether users: the City Age was over.\n",
            "In the years since the City Age the Vault's activities have continued apace, attracting Guardians from across the system to keep watch on vital information. The Guardian-Servitor Ghost Fragment notes that at least four factions have joined the effort, including the Guardians who have been tracking Fallen activity in recent weeks.\n",
            "Another large-scale effort, involving multiple factions with distinct agendas, is underway, maintaining records of Fallen activity in the system's Golden Age colonies. The Guardian-Servitor and Warlord Warlock, both high-ranking members of the Warlock House, have agreed to cooperate, but will they accept help compiling and managing data for a unified effort? Will the Vault continue to operate despite all Guardians' defeats? Or will the Guardians' mutual interest in maintaining the stability of the Golden Age and the survival of the Golden Age's Golden Age allies drive them to do anything in their power to dislodge Fallen elements from the system and isolate those elements who see no future in the system?\n",
            "There is no telling what will follow. It is hard to imagine a future that does not end with the extinction of the Fallen from our world. \n",
            "In conclusion, I write this today as I close my eyes to recover.  My efforts have paid off.  Ether has never been more valuable to me. As a result, I have learned to take more care in managing an environment as unpredictable as the City Age.  In the years ahead, I will continue my studies in space and time.  The City Age will reach a high point\n",
            "\n",
            "[210 | 504.85] loss=1.71 avg=2.61\n",
            "[220 | 528.20] loss=1.85 avg=2.57\n",
            "[230 | 551.57] loss=1.59 avg=2.52\n",
            "[240 | 574.97] loss=1.38 avg=2.47\n",
            "[250 | 598.36] loss=1.57 avg=2.43\n",
            "[260 | 621.77] loss=1.62 avg=2.39\n",
            "[270 | 645.18] loss=1.20 avg=2.34\n",
            "[280 | 668.62] loss=1.27 avg=2.30\n",
            "[290 | 692.00] loss=0.96 avg=2.25\n",
            "[300 | 715.42] loss=1.09 avg=2.20\n",
            "[310 | 738.83] loss=0.89 avg=2.15\n",
            "[320 | 762.25] loss=1.04 avg=2.11\n",
            "[330 | 785.66] loss=1.30 avg=2.08\n",
            "[340 | 809.08] loss=0.89 avg=2.04\n",
            "[350 | 832.49] loss=0.81 avg=2.00\n",
            "[360 | 855.93] loss=0.69 avg=1.96\n",
            "[370 | 879.32] loss=0.77 avg=1.92\n",
            "[380 | 902.74] loss=0.96 avg=1.89\n",
            "[390 | 926.13] loss=0.68 avg=1.85\n",
            "[400 | 949.51] loss=0.59 avg=1.81\n",
            "======== SAMPLE 1 ========\n",
            "x1); (x2); eu; delta++, delta++; es...\n",
            "\n",
            "\n",
            "Ghost Fragment: Exodus\n",
            "The Red Ships\n",
            "I am Cayde-6, Commander of the Exodus Deck-Lead. Now. Now.\n",
            "I am a gifted navigator, and you will accept my challenge.\n",
            "Begin by clearing the two largest harbors on the planet: our objective is to reach them, and then take the three remaining creeks and take the cisjamb home.\n",
            "Each ship has its own unique tactics. The Corsair has unimpeachable accuracy, while the Titan and the Phalanx have no doubting your enemy's strengths. Of six propulsion systems aboard the Exodus, only the Phalanx is down. I hold my position, and now... I am neither brave nor greedy.\n",
            "The Crows are as stubborn as ever, but they still refuse to leave.\n",
            "The Phalanx is indecisive and slow. If we are lucky, it may be pushing us back. If I am right, then it may have us locked up.\n",
            "I am hoping for the best. I will ask for your help.\n",
            "Vell: What protocol do you use to enter and leave the Cabal shipyards? Eris: Simple. Run a red light. Get cover. The Red Ships will kill us all.\n",
            "Eriana: It doesn't matter. I am the only Guardian with a weapon with the terminal to kill us all.\n",
            "Eriana: Any protocol changed? Cayde?\n",
            "Eriana: Simple. Just fire a couple of bolts into the night. Nothing fatal here.\n",
            "Eriana: The Fallen? No questions asked.\n",
            "Eriana: Not in the name of—\" Banners? Cards? \" Tho' this be enough?\n",
            "Eriana: What's the point of protocol? Anymore, if you're going to make friends, you have to make friends—\n",
            "Eriana: You're making a Brigand, aren't you? This is my badge of honor. You see my Light in all of you, and in every badge you wears.\n",
            "Eriana: True. But—and this is the last word.\n",
            "Eriana: Not here! I—\n",
            "Eriana: You're welcome.\n",
            "\n",
            "\n",
            "Ghost Fragment: Exodus 2\n",
            "\n",
            "Eriana, Arcite Inferno, Shim, I can do this.\n",
            "Eriana, Shim, I can do this.\n",
            "Eriana, Shim, I can do this.\n",
            "Eriana, Shim, I can do this.\n",
            "Eriana, Shim, we have to act now.\n",
            "Eriana, Shim, we have to act quickly.\n",
            "Eriana, Shim, we have to act now.\n",
            "Eriana, Shim, there must be a way.\n",
            "Eriana, Shim, there must be a way.\n",
            "Eriana, Shim, we have to act now.\n",
            "Eriana, Shim, we have to act now.\n",
            "Eriana, Shim, we have to act now.\n",
            "\n",
            "\n",
            "The Cabal, Son of Crota and Xod - 2nd Darkness\n",
            "\n",
            "&lt;transmission 800w-amplifier \"USB HID API HARD\" \"usb-acquire 400\" \"usb-recoil 400\" \"auto qemu-s3\" \"auto aciuse\" \"usb-recoil 400\" \"auto qemu-s4\" \"auto abjuration\" \"automate\" \"audible-exit\"\n",
            "&lt;//transmission800w-amplified \"USB HID API HARD\" \"usb-acquire 400\" \"usb-recoil 400\" \"auto qemu-s3\" \"auto aciuse\" \"vigorously remake\" \"audible-exit\"\n",
            "&lt;//transmission900w-arrival-intruder \"auto qemu-s3\" \"auto aciuger clear\" \"audible-exit\"\n",
            "&lt;//intercepted \"automate\" \"audible-exit\"\n",
            "Eriana-061:  Yeah. It's over.\n",
            "Arcite's detected a human being living in a cut space on the Martian moon. It's Titan, you know. \n",
            "Eriana, I think you might be able to make it to the human side of the border.\n",
            "Eris: Yeah. We've got a plan.\n",
            "Titan: It's not us—\n",
            "Arcite: It's fine. We don't have to kill each other. Just—\n",
            "Eriana-061:  I don't need to.\n",
            "Eris: In a way. It's just— I don't understand.\n",
            "Titan: It can—it can. We're on its way. Won't need it—\n",
            "Eriana-061:\n",
            "\n",
            "[410 | 983.07] loss=0.86 avg=1.79\n",
            "[420 | 1006.43] loss=0.45 avg=1.75\n",
            "[430 | 1029.80] loss=0.45 avg=1.71\n",
            "[440 | 1053.15] loss=0.46 avg=1.68\n",
            "[450 | 1076.53] loss=0.45 avg=1.64\n",
            "[460 | 1099.91] loss=0.25 avg=1.60\n",
            "[470 | 1123.29] loss=0.26 avg=1.57\n",
            "[480 | 1146.68] loss=0.34 avg=1.54\n",
            "[490 | 1170.08] loss=0.30 avg=1.50\n",
            "[500 | 1193.46] loss=0.29 avg=1.47\n",
            "Saving checkpoint/run1/model-500\n",
            "[510 | 1219.34] loss=0.30 avg=1.44\n",
            "[520 | 1242.81] loss=0.30 avg=1.42\n",
            "[530 | 1266.22] loss=0.35 avg=1.39\n",
            "[540 | 1289.59] loss=0.36 avg=1.37\n",
            "[550 | 1312.91] loss=0.19 avg=1.34\n",
            "[560 | 1336.23] loss=0.18 avg=1.31\n",
            "[570 | 1359.61] loss=0.19 avg=1.29\n",
            "[580 | 1383.06] loss=0.16 avg=1.26\n",
            "[590 | 1406.51] loss=0.25 avg=1.24\n",
            "[600 | 1429.93] loss=0.15 avg=1.21\n",
            "======== SAMPLE 1 ========\n",
            " Queen? We have a chance to retake the Yucatán Sector at the cost of countless Fallen lives.\n",
            "\n",
            "\n",
            "The Fall of Ishtar Sink\n",
            "\n",
            "Who was this terrible voice? It seemed as if a sweep of the Awoken Tower with the distant Devil Splicers brought silence. \n",
            "We knew they were listening - to the rumbling, the alien entanglements that were the Devils. \n",
            "It seemed as though a powerful unknown - perhaps the Vex - policed the whole area. \n",
            "In the weeks since, rumors of the Splicers' arrival have spread that... anyone who sees them will be very, very angry. \n",
            "That angry, very much may already be sating their hunger with  the information the Vanguard provides. \n",
            "Will the Vanguard hold...? \n",
            "REPEALMENT: \n",
            "The Vanguard has confirmed multiple times that the Fallen are everywhere. In finding lost Ordis, the Vanguard is \"incapating the Fallen with [their disgust]. We're doing our part to deter any more aggression.\"\n",
            "The truth is, the Fallen are not so quick to understand each other. While we attempt to work out what the Fallen think of each other's quirks, it is important to understand one another so we can work through each other.\n",
            "REPEALMENT:\n",
            "\n",
            "The situation on the Ordo Malok front is sketchy. We are stuck in contact via word from the Plaguelands, as well as anonymous reports from Fallen. There are also reports of a Ju-4 out and about, firing live rounds. It seems the Vanguard is stuck in a dead swing, though we are unclear on the nature of Ordo's weapon. Even Gjoni reports seeing \"ghosts\" near the Ordo's. There is anger in the high seas - all factions angry at the state of the Ordo's standing in the real world.\n",
            "REPEALMENT: not yetiable task. SECRET EXTRA: ORDIAN SPACE HAT\n",
            "BY AIR\n",
            "V6: MASS-RD REEL\n",
            "[u.2:55] A [u.1:56] General Task Force ORD launched resupplies towards a Hive Hive vessel. \n",
            "Two [uu.3:57] Fallen [u.2:58] Hobgoblins arrived with attacks converging on Ordo's Keep. \n",
            "The RCP was destroyed, the House of Ordo was broken, and the Haven’s fleet deployed. TF agreed to the Ordo’s demands for continued support.\n",
            "During a lull, a small TF force was able to breach the hold at the eastern end of the Ordo’s Keep. Rangers, supported by infantry, attacked the runs and broke into the TF line of attack to liberate the High King’s prison.\n",
            "The Hobgoblins were repelled and the Hobgoblins were reduced to few survivors. TF commander Colovance then went on record requesting that TF destroy the Ordis’s fleet to allow time for regaining contact with the Hive. \n",
            "After widespread protests from the Hobgoblins, the Commander-in-Chief rescinded this request on the grounds that the Hobgoblins were too aggressive, and instead directed all TF forces to defend their Ordo’s Keep.\n",
            "On May 3, 1567, the last of Ordis’s Kell troops left Ordo's Keep, Ordis flew back to Durga, and the Kell forces opened fire.\n",
            "The initial deafening silence of the House of Wolves' battle echoed through the Dreadnaught, and the sound of battle reigned.\n",
            "The Kell held their ground, holding back the combined forces of Ordis, Hobgoblin, and Skiff. The Silent Fang broke ranks, charging back toward the Kells.\n",
            "A Ketch dive bomb dropped on Ordis' position killed the Kell at the last second.\n",
            "Rezyl’s Ketch received a low concuss and, realizing its sub-optimal firing order had been broken, ordered all Kells to converge on the Ecumene gate.\n",
            "Without warning, the Fang set to conduct a general strike. They gathered at the Kells' gate, eyes burning, as dawn fell.\n",
            "Rezyl marched his troops as close to the Ecumene stronghold as possible, but the lightning didn’t leave any opportunities. He crossed the bounding area, and took in the view, before turning left and heading for the Kells' shock core.\n",
            "When the core had struck him, Rezyl jumped off the Ketch and launched a focused kick, blasting the core’s core’closed. He landed on a ruined plateau, a rolling sierra of broken mountains spread across a valley.\n",
            "Rezyl landed next to a series of broken peaks, just out of the contact range of one of the peaks. The shock wave was what he needed: an earthquake.\n",
            "The Ecumene had been destroyed.\n",
            "The\n",
            "\n",
            "[610 | 1463.68] loss=0.16 avg=1.19\n",
            "[620 | 1487.05] loss=0.18 avg=1.17\n",
            "[630 | 1510.43] loss=0.14 avg=1.15\n",
            "[640 | 1533.80] loss=0.15 avg=1.13\n",
            "[650 | 1557.17] loss=0.13 avg=1.10\n",
            "[660 | 1580.56] loss=0.15 avg=1.09\n",
            "[670 | 1603.95] loss=0.14 avg=1.07\n",
            "[680 | 1627.33] loss=0.10 avg=1.05\n",
            "[690 | 1650.70] loss=0.12 avg=1.03\n",
            "[700 | 1674.04] loss=0.10 avg=1.01\n",
            "[710 | 1697.41] loss=0.16 avg=0.99\n",
            "[720 | 1720.78] loss=0.11 avg=0.98\n",
            "[730 | 1744.16] loss=0.09 avg=0.96\n",
            "[740 | 1767.52] loss=0.10 avg=0.94\n",
            "[750 | 1790.89] loss=0.12 avg=0.93\n",
            "[760 | 1814.25] loss=0.11 avg=0.91\n",
            "[770 | 1837.60] loss=0.09 avg=0.90\n",
            "[780 | 1860.97] loss=0.12 avg=0.88\n",
            "[790 | 1884.29] loss=0.10 avg=0.87\n",
            "[800 | 1907.61] loss=0.08 avg=0.85\n",
            "======== SAMPLE 1 ========\n",
            " pressure by the Hive in a final struggle. We are still out beyond the Eyes, more Ghosts than Cabal, but still in the Eyes. We will be there long after.\" - Ikora Rey\n",
            "\n",
            "\n",
            "Bracus Horu'usk, Exile\n",
            "\"No one showed us they're for real. Haven't seen them yet. They're not supposed to be out there, out there. All we saw were horrifying, awful things. We didn't anticipate being attacked. My mother was a huge Runel oran, war-machINE fan. She was huge on Ikora Rey's shoulders. She always made a point to have her nose around everything. It was her smile that made us feel close. My father was an electrician, and his face was painted quite like Horu’usk. I'm pretty sure it's just the smoke and mirrors of his vents. -Poppy\n",
            "\"My Father Was An Unbelievable\" - The Asmodia\n",
            "\n",
            "\n",
            "The Sunbreaker's Challenge\n",
            "\n",
            "\"Guy has a plan. Nothing else. And it works. Think of how many Runner he'll beat on his journey.\"\n",
            "&gt;o fa&gt;r&gt;o\n",
            "&gt;u&lt;interdict&gt;undread\n",
            "&gt;u&lt;force frisbee\n",
            "&gt;u&lt;beat the Runner down\n",
            "&gt;u&lt;beat the Runner TRUE\n",
            "&gt;u&lt;beat the Runner down FALSE\n",
            "&gt;u&lt;or find a new path\n",
            "&gt;u&lt;kills the Runner TRUE\n",
            "&gt;u&lt;inspires ultimate goal\n",
            "&gt;u&lt;to achieve ultimate destruction\n",
            "&gt;u&lt;kill Runner TRUE\n",
            "&gt;u&lt;beat the Runner down ultimate evil\n",
            "&gt;u&lt;never again\n",
            "&gt;u&lt;rrgroup target TRUE\n",
            "&gt;u&lt;inside image of abducted Runner\n",
            "&gt;u&lt;beat the targeted Guardian subroutine\n",
            "&gt;u&lt;see direct cut\n",
            "&gt;u&lt;outcome will be determined by will\n",
            "&gt;u&lt;Kill Guardian subroutine goal\n",
            "&gt;u&lt;specified method of killing vanguard\n",
            "&gt;u&lt;unlock ultra-rare ability\n",
            "&gt;u&lt;repair drone\n",
            "&gt;u&lt;replicate Vex strength to destroy flank\n",
            "&gt;u&lt;replicate topography more suited to Guardian training\n",
            "&gt;u&lt;raise general by storm\n",
            "&gt;u&lt;SHIELD AI detects Guardian fleet strength 'stealing' objective\n",
            "&gt;u&lt;diagnose and punish Vex for offensive and defensive purposes\n",
            "&gt;u&lt;share numerical advantages\n",
            "&gt;u&lt;emulate compact rationality\n",
            "&gt;u&lt;NO CONNECTION WITH GUARDIANS~\n",
            "&gt;UAV FORCECON V6NNX~\n",
            "&gt;enable Guardian fleet defense against Vex threats\n",
            "&gt;u&lt;IMMUNIZE AUTHORIZED AID VECCHASER ~\n",
            "&gt;u&lt;SHUT IT DOWN~\n",
            "\n",
            "\n",
            "The Taken War: Venus\n",
            "\n",
            "&gt;From the Eliksni Declaration :\n",
            "  1. The Council must pursue the Fundament Apocalypse as aggressively as we can.  2. The Vanguard cannot rule out the possibility that the Cabal strike group in question are involved in a Vex-eliminated place, place, place, place, place, place, place, place.  3. The Hive fleet segment must avoid raising the Fundament at the earliest opportunity.\n",
            "&gt;In any case, proceed with caution.  4. The House of Wolves has requested that we halt the raising of the Fundament until further notice. If feasible, I hereby declare the Primus Ta'aun as your commando and appoint an amicable friend to assist you.\n",
            "&gt;Wolf orders!\n",
            "&gt;Yours, and every other competition, except your own.  5. The Queen's bounty on you is now open, and I —\n",
            "&gt;To make Guardians of the Reef, I will do anything —\n",
            "&gt;To assist in the suppression of the Queen, I will do anything —\n",
            "&gt;To assist in the suppression of the Queen, I will do anything.  6. By my order, the Queen's auction of weapons has been canceled, and my agents have been dispatched to pursue those who wield the Vex power.\n",
            "&gt;In this interloper's interest, I have ordered that their names be called for Targaryen, Radegast, and a later quantity.\n",
            "&gt\n",
            "\n",
            "[810 | 1941.10] loss=0.09 avg=0.84\n",
            "[820 | 1964.40] loss=0.12 avg=0.83\n",
            "[830 | 1987.70] loss=0.08 avg=0.81\n",
            "[840 | 2011.01] loss=0.07 avg=0.80\n",
            "[850 | 2034.34] loss=0.08 avg=0.79\n",
            "[860 | 2057.70] loss=0.06 avg=0.78\n",
            "[870 | 2081.02] loss=0.09 avg=0.76\n",
            "[880 | 2104.37] loss=0.10 avg=0.75\n",
            "[890 | 2127.74] loss=0.09 avg=0.74\n",
            "[900 | 2151.11] loss=0.09 avg=0.73\n",
            "[910 | 2174.48] loss=0.08 avg=0.72\n",
            "[920 | 2197.83] loss=0.08 avg=0.71\n",
            "[930 | 2221.19] loss=0.08 avg=0.70\n",
            "[940 | 2244.52] loss=0.12 avg=0.69\n",
            "[950 | 2267.85] loss=0.09 avg=0.68\n",
            "[960 | 2291.20] loss=0.10 avg=0.67\n",
            "[970 | 2314.56] loss=0.08 avg=0.66\n",
            "[980 | 2337.90] loss=0.09 avg=0.65\n",
            "[990 | 2361.21] loss=0.10 avg=0.64\n",
            "[1000 | 2384.53] loss=0.11 avg=0.63\n",
            "Saving checkpoint/run1/model-1000\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXSuTNERaw6K"
      },
      "source": [
        "After the model is trained, you can copy the checkpoint folder to your own Google Drive.\n",
        "\n",
        "If you want to download it to your personal computer, it's strongly recommended you copy it there first, then download from Google Drive. The checkpoint folder is copied as a `.rar` compressed file; you can download it and uncompress it locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3"
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJgV_b4bmzd"
      },
      "source": [
        "You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L"
      },
      "source": [
        "## Load a Trained Model Checkpoint\n",
        "\n",
        "Running the next cell will copy the `.rar` checkpoint file from your Google Drive into the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD"
      },
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name='run1')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTa6zf3e_9gV"
      },
      "source": [
        "The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='run1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp"
      },
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff2d7fa8-ecd4-4491-e1de-6c65585cb488"
      },
      "source": [
        "gpt2.generate(sess, run_name='run1')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1st Place: Golden Age Games\n",
            "\n",
            "2nd Place: The Traveler\n",
            "\n",
            "3rd Place: Petra\n",
            "“Whoever built the Traveler”\n",
            "4th Place: That’s a master plan.\n",
            "“I hope you’re stronger now.”\n",
            "\n",
            "\n",
            "Ghost Fragment: The Traveler\n",
            "Dreams of Alpha Lupi\n",
            "The weapon used by the Traveler in its final, primal form.\n",
            "The weapon used by the Traveler in its final, primal form. \n",
            "The weapon’s mechanism is a mix of fossilized matter and ancient machinery. It has a delicate balance between calm and madness.\n",
            "\n",
            "\n",
            "Ghost Fragment: The Traveler 2\n",
            "Dreams of Alpha Lupi\n",
            "The weapon used by the Traveler in its final, primal form.\n",
            "The weapon used by the Traveler in its final, primal form. \n",
            "The mechanism is thought to be a combination of six-dimensional living organisms’ minds and a basic understanding of space and time.\n",
            "\n",
            "\n",
            "Ghost Fragment: The Traveler 3\n",
            "Dreams of Alpha Lupi\n",
            "The weapon used by the Traveler in its final, primal form.\n",
            "The weapon used by the Traveler in its final, primal form. \n",
            "The mechanism is thought to be a combination of six-dimensional living organisms’ minds and a basic understanding of time.\n",
            "\n",
            "\n",
            "\n",
            "The Sword\n",
            "\"The Sword conquers.\"\n",
            "The Sword is the embodiment of death.\n",
            "The sword's mechanism is cold and transparent, yet it has a blunt force to it that is also blunt. It has a body so round it almost seems like a child's toy, except that it is not.\n",
            "The sword is a living thing, part of the universe created by the Traveler. It has no mind or meaning to it.\n",
            "\n",
            "\n",
            "The Sword: \n",
            "The Sword is the outward expression of the sword-sinking power of the sword.\n",
            "The Sword is a crown made of the sword and a small jewel, left by the wielder in thanks.\n",
            "The wielder is the ruler of the crown. The sword's purpose is the worship of death, and the desire for totality.\n",
            "\n",
            "\n",
            "The Sword:ilon\n",
            "\"The Sword flows into the World-Render. It is the violence of space.\"\n",
            "The Sword:ilon is the sharpened blade like an axe.\n",
            "Its mechanism is the World-Render, the shallow, brooding bit of the sword's metal that has been cut away by the World-Render. It has a beautiful, sharp edge to it, unlike any other sword on the face of the universe.\n",
            "The sword is a living thing, cut from the mithril world. It has a purpose.\n",
            "\n",
            "\n",
            "The Sword:jul3\n",
            "\"If it ever comes to pass, I will grant it immortality, by the sword of my own will.\"\n",
            "The Sword:jul3 is the sharpened edge by which the sword's metal is shredded.\n",
            "The sword's mechanism is the world-ender coming to an end, by which time the world's Light has been transmuted into an Auto-Wield. You see, the Sword makes an ideal weapon for crushing down anything that might fall into its clutches.\n",
            "\n",
            "\n",
            "The Sword:mar3\n",
            "\"If it ever comes to pass, I will grant it eternal, by the sword of my own will.\"\n",
            "The Sword:mar3 is the sharpened edge by which the sword's metal is tempered.\n",
            "It has been used as a cutting tool in the right hands, as a blade catch for sharpened weapons, and as a makeshift killing instrument.\n",
            "\n",
            "\n",
            "The Sword:mar6\n",
            "\"At the sword's end lies the peace of your Land.\"\n",
            "The Sword:mar6 is the edge by which the sword's metal is hammered.\n",
            "It has been claimed that the Sword was created from the molten wreckage of a lost ship, and that it winds down into the ocean to be used as a bargaining chip to craftil.\n",
            "The Sword is native to the Reef and Tarlowe Seas. It is believed to live among the Reef's outposts.\n",
            "There are countless threats to the peace of the Reef. To deal with these dangers requires integrity, and the Sword is one of those pieces of hardware.\n",
            "Steel, brass, and diamond are the tools of the trade. Hunters and Wizards alike will use them to craft powerful weapons. Master their power and you might make a weapon that rivals the Sword...only you can answer that question.\n",
            "\n",
            "\n",
            "The Sword:mar7\n",
            "Steel\n",
            "\n",
            "The sword is steel, dipped into the deepest depths of the Vex. Nothing else does.\n",
            "There are those who believe that the wielder of a weapon is a natural at heart. This is not the case. Every weapon is designed to be honed and perfected over a long lifecycle. Every weapon is engineered to withstand the elements. Every weapon is engineered to shine in the moment of wielder's hand.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R"
      },
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
        "\n",
        "You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
        "\n",
        "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
        "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
        "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "526dbd1b-f076-4f47-a42b-058ab4a35b9f"
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              length=250,\n",
        "              temperature=0.7,\n",
        "              prefix=\"LORD\",\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LORD WILLOUGHBY:\n",
            "That, by the way, Clarence and I have done good side by side;\n",
            "And yet side we, and he side we have done ill.\n",
            "\n",
            "KING RICHARD II:\n",
            "Why then 'tis done ill. O, how should I ease it?\n",
            "Side with him and my brother, my sovereign!\n",
            "Side wither away, and as night falls,\n",
            "Like to the farthest morning to my last,\n",
            "Side wither away, and as morning comes,\n",
            "Like to the furthest afternoon to my last!\n",
            "Side wither away, and as our fortunes turn,\n",
            "Like to the furthest afternoon to our last!\n",
            "\n",
            "QUEEN MARGARET:\n",
            "What is this? counsel? counsel!\n",
            "\n",
            "KING RICHARD II:\n",
            "My queen and my heir, for half a mile and a half\n",
            "She will glide this way, to be or no.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "So stands the orchard here, for half a mile and a!\n",
            "\n",
            "KING RICHARD II:\n",
            "So stands the orchard here, to fence it, to!\n",
            "Fashion it in her, like the hedgehog's net\n",
            "====================\n",
            "LORD STANLEY:\n",
            "What if I told you, in the hope of succor,\n",
            "That I had lain a little while in your arms?\n",
            "\n",
            "DUKE OF YORK:\n",
            "No doubt, my lord.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "'Tis a pity I should be coil'd to\n",
            "Be brief and unanswerable. Yet give me this.\n",
            "\n",
            "EXTON:\n",
            "'Tis a truth that vexes me deeply\n",
            "To try whether thou, Lord Hastings, art moved\n",
            "To enter publicly with gentle discourse\n",
            "And thanks from his acknowledged friends.\n",
            "\n",
            "HASTINGS:\n",
            "My gracious lord,\n",
            "Suppose me this: did I so love to see the Tower?\n",
            "\n",
            "KING RICHARD II:\n",
            "I did so; but the duelling Tower, moved\n",
            "By jealousies to oppress me,\n",
            "Wretches to usurp him held most dear,\n",
            "The truth is, I loved the Tower as I loved\n",
            "The princes that envied their prosperity.\n",
            "\n",
            "HASTINGS:\n",
            "I loved innocently,\n",
            "When my princes did usurp their gains; when\n",
            "When my grandsire and my liege, prince and prince,\n",
            "B\n",
            "====================\n",
            "LORD WILLOUGHBY:\n",
            "How long shall it take? only\n",
            "To behold your father's bending in the duke's.\n",
            "How long shall it be? do you understand me?\n",
            "\n",
            "ROMEO:\n",
            "Your grace, I understand you.\n",
            "\n",
            "ROMEO:\n",
            "It must be so; then I'll excuse myself.\n",
            "\n",
            "ROMEO:\n",
            "It shall be so.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Your nose is pleasant on myself.\n",
            "\n",
            "ROMEO:\n",
            "No need.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "My heart is troubled by strange thoughts.\n",
            "\n",
            "ROMEO:\n",
            "No need.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "My heart is troubled by strange thoughts.\n",
            "\n",
            "ROMEO:\n",
            "Prithee, be quiet; thou need'st it.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "My mind is unsettled; no need.\n",
            "\n",
            "ROMEO:\n",
            "Prithee, be quiet; thou need'st it.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "My mind is unsettled; no need.\n",
            "\n",
            "ROMEO:\n",
            "Prithee, be quiet; thou need'st it.\n",
            "\n",
            "FRIAR LA\n",
            "====================\n",
            "LORD WILLOUGHBY:\n",
            "And, if the right Edward were slain,\n",
            "My father's blood should wash the world from me.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "O, let him fly from me, that he may live!\n",
            "\n",
            "PRINCE EDWARD:\n",
            "Arise, one last, and let him be slain ere he return.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "I thank God I am not young nor old to waste.\n",
            "I am young and wooer than this young wooer was.\n",
            "\n",
            "YORK:\n",
            "Younger than young, and wooer than a man is.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "What, wilt thou not kill her?\n",
            "\n",
            "RYBHUS:\n",
            "If thou darest, thou hast to do good deeds,\n",
            "If thou darest, thou darest not kill her, thou darest.\n",
            "\n",
            "PRINCE EDWARD:\n",
            "Where dost thou go? command Warwick to take her?\n",
            "\n",
            "WARWICK:\n",
            "Where leadess Warwick be that Warwick is woo'd.\n",
            "\n",
            "YORK:\n",
            "Where leadess Warwick be woo'd that noble York is.\n",
            "\n",
            "PRINCE\n",
            "====================\n",
            "LORD WILLOUGHBY:\n",
            "Let him please to come and sup with him?\n",
            "\n",
            "WARWICK:\n",
            "I promised he should come and sup with him.\n",
            "\n",
            "YORK:\n",
            "'Twas a vow of charity to vex him,\n",
            "And then he should vex us to come and sup.\n",
            "\n",
            "WARWICK:\n",
            "'Twas but a vow to come and sup with him.\n",
            "\n",
            "YORK:\n",
            "'Twas but a vow to come and sup with him.\n",
            "\n",
            "WARWICK:\n",
            "Come hither, slave boy.\n",
            "Me I come, you wretched hag, you wretched thing.\n",
            "\n",
            "DORCAS:\n",
            "'Tis very well. Come, go with me.\n",
            "\n",
            "WARWICK:\n",
            "I will be his slave, and make his bondage known.\n",
            "\n",
            "EXTON:\n",
            "So, you have resisted his bondage, you have run your errand too far.\n",
            "\n",
            "WESTMORELAND:\n",
            "O, but O, the slave that was past the year\n",
            "Doth not my errand a better errand?\n",
            "\n",
            "EXTON:\n",
            "No, for so my wits charge me thus too late.\n",
            "\n",
            "WARWICK:\n",
            "But\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2"
      },
      "source": [
        "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0"
      },
      "source": [
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-LRex8lfv1g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "dcad5e73-89fc-41fc-f3c1-feaf963f0ce1"
      },
      "source": [
        "# may have to run twice to get file to download\n",
        "files.download(gen_file)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_10634c37-bf51-4972-9c67-761132da60a4\", \"gpt2_gentext_20210207_025423.txt\", 195479)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQAN3M6RT7Kj"
      },
      "source": [
        "## Generate Text From The Pretrained Model\n",
        "\n",
        "If you want to generate text from the pretrained model, not a finetuned model, pass `model_name` to `gpt2.load_gpt2()` and `gpt2.generate()`.\n",
        "\n",
        "This is currently the only way to generate text from the 774M or 1558M models with this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsUd_jHgUZnD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "4e0c8a3f-3527-41c4-e3fe-3357f3f8f6c2"
      },
      "source": [
        "model_name = \"774M\"\n",
        "\n",
        "gpt2.download_gpt2(model_name=model_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 354Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 131Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 279Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 3.10Git [00:23, 131Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 380Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 2.10Mit [00:00, 226Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 199Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAe4NpKNUj2C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "b09bfe1d-2ff8-4b8a-fffb-273d28d5d4ae"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.load_gpt2(sess, model_name=model_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0828 18:37:58.571830 139905369159552 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading pretrained model models/774M/model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xInIZKaU104",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "outputId": "56348e28-7d08-45e3-c859-f26c0efd066d"
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              model_name=model_name,\n",
        "              prefix=\"The secret of life is\",\n",
        "              length=100,\n",
        "              temperature=0.7,\n",
        "              top_p=0.9,\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The secret of life is that it's really easy to make it complicated,\" said Bill Nye, the host of the popular science show \"Bill Nye the Science Guy.\" \"And this is one of the reasons why we all need to be smarter about science, because we can't keep up with the amazing things that are going on all the time.\"\n",
            "\n",
            "While Nye is correct that \"everything that's going on all the time\" is making the world a better place, he misses the point. This is not\n",
            "====================\n",
            "The secret of life is in the rhythm of the universe. It's not a mystery. It's not a mystery to me. It's the nature of the universe. It's the beauty of the universe. It's the way the universe works. It's the way the universe is. It's the way the universe is going to work. It's the way the universe is. It's the way the universe is. It's the way the universe is. It's the way the universe is. It's the way\n",
            "====================\n",
            "The secret of life is in the universe.\n",
            "\n",
            "\n",
            "-\n",
            "\n",
            "The Red Devil\n",
            "\n",
            "It's the end of the world as we know it, and the only thing that can save us is a band of super-powered individuals known as the Red Devil.\n",
            "\n",
            "\n",
            "The Red Devil is a group of super-powered individuals who are seeking the secret of life and the only way they know how to do it is by taking on the roles of a variety of different super-powered individuals, each of which has their own\n",
            "====================\n",
            "The secret of life is in the mixing of the elements, and it is the mixing of the elements that makes life possible.\"\n",
            "\n",
            "But in the world of food science, the idea of a \"complex\" or \"complexity\" is almost entirely imaginary.\n",
            "\n",
            "As a scientist, I'm fascinated by the question of how life first began.\n",
            "\n",
            "It's the question that drives my work and the work of the scientists who work on it.\n",
            "\n",
            "My current research is exploring how microbes work in the first moments\n",
            "====================\n",
            "The secret of life is the journey of life, the search for the truth.\n",
            "\n",
            "4.4.2. The last thing you know\n",
            "\n",
            "There is nothing more important than the last thing you know.\n",
            "\n",
            "4.4.3. The little things that make all the difference\n",
            "\n",
            "The little things that make all the difference.\n",
            "\n",
            "4.4.4. The truth is the best teacher\n",
            "\n",
            "The truth is the best teacher.\n",
            "\n",
            "4.4.5. The truth is what\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-KVgkCDCKD"
      },
      "source": [
        "# Etcetera\n",
        "\n",
        "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIHiVP53FnsX"
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmTXWNUygS5E"
      },
      "source": [
        "# LICENSE\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 Max Woolf\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    }
  ]
}